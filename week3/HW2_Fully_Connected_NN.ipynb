{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Полносвязная нейронная сеть\n",
    "В данном домашнем задании вы подготовите свою реализацию полносвязной нейронной сети и обучите классификатор на датасете CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet(object):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
    "        \"\"\"\n",
    "        W1: Первый слой с размерами (D, H)\n",
    "        b1: Вектор байесов, размер  (H,)\n",
    "        W2: Второй слой с размерами (H, C)\n",
    "        b2: Вектор байесов, размер (C,)\n",
    "\n",
    "        Входные парамерты:\n",
    "        - input_size: Размерность входных данных\n",
    "        - hidden_size: Размер скрытого слоя\n",
    "        - output_size: Количество классов\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "        self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def loss(self, X, y=None, reg=0.0):\n",
    "        \"\"\"\n",
    "        Вычисление функции потерь\n",
    "\n",
    "        Входные парамерты:\n",
    "        - X: Таблица данных (N, D). X[i] - один пример\n",
    "        - y: Вектор лейблов. Если отсутсвует, то возвращается предсказание лейблов\n",
    "        - reg: Коэффициент регуляризации\n",
    "\n",
    "        Возвращает:\n",
    "        Если y == None, то возвращаются скор для классов\n",
    "\n",
    "        Если y != None, то возвращаются:\n",
    "        - Лосс для данного семпла данных\n",
    "        - grads: Словарь градиентов, ключи соответствуют ключам словаря self.params.\n",
    "        \"\"\"\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        N, D = X.shape\n",
    "\n",
    "        scores = None\n",
    "        #############################################################################\n",
    "        # TODO: Расчет forward pass или прямой проход, для данных находятся скоры,  #\n",
    "        # на выходе размер (N, C)                                                   #\n",
    "        #############################################################################\n",
    "        pass\n",
    "        #############################################################################\n",
    "        #                              END OF YOUR CODE                             #\n",
    "        #############################################################################\n",
    "\n",
    "        # Если y == None, то завершаем вызов\n",
    "        if y is None:\n",
    "            return scores\n",
    "\n",
    "        loss = None\n",
    "        #############################################################################\n",
    "        # TODO: Расчет Softmax loss для полученных скоров обьектов, на выходе скаляр        #\n",
    "        #############################################################################\n",
    "        pass\n",
    "        #############################################################################\n",
    "        #                              END OF YOUR CODE                             #\n",
    "        #############################################################################\n",
    "\n",
    "        grads = {}\n",
    "        #############################################################################\n",
    "        # TODO: Расчет обратнохо прохода или backward pass, находятся градиенты для всех    #\n",
    "        # параметров, результаты сохраняются в grads, например grads['W1']               #\n",
    "        #############################################################################\n",
    "        pass\n",
    "        #############################################################################\n",
    "        #                              END OF YOUR CODE                             #\n",
    "        #############################################################################\n",
    "\n",
    "        return loss, grads\n",
    "    \n",
    "\n",
    "    def train(self, X, y, X_val, y_val,\n",
    "                learning_rate=1e-3, learning_rate_decay=0.95,\n",
    "                reg=5e-6, num_iters=100,\n",
    "                batch_size=200, verbose=False):\n",
    "        \"\"\"\n",
    "        Обучение нейронной сети с помощью SGD\n",
    "\n",
    "        Входные парамерты:\n",
    "        - X: Матрица данных (N, D)\n",
    "        - y: Вектор лейблов (N, )\n",
    "        - X_val: Данные для валидации (N_val, D)\n",
    "        - y_val: Вектор лейблов валидации (N_val, )\n",
    "        - reg: Коэффициент регуляризации\n",
    "        - num_iters: Количнство итераций\n",
    "        - batch_size: Размер семпла данных, на 1 шаг алгоритма\n",
    "        - verbose: Вывод прогресса\n",
    "        \"\"\"\n",
    "        num_train = X.shape[0]\n",
    "        iterations_per_epoch = max(num_train / batch_size, 1)\n",
    "\n",
    "        loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "\n",
    "            #########################################################################\n",
    "            # TODO: Семпл данных их X-> X_batch, y_batch                              #\n",
    "            #########################################################################\n",
    "            pass\n",
    "            #########################################################################\n",
    "            #                             END OF YOUR CODE                          #\n",
    "            #########################################################################\n",
    " \n",
    "            loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            #########################################################################\n",
    "            # TODO: Используя градиенты из grads обновите параметры сети                 #\n",
    "            #########################################################################\n",
    "            pass\n",
    "            #########################################################################\n",
    "            #                             END OF YOUR CODE                          #\n",
    "            #########################################################################\n",
    "\n",
    "            if verbose and it % 100 == 0:\n",
    "                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "\n",
    "            if it % iterations_per_epoch == 0:\n",
    "                train_acc = (self.predict(X_batch) == y_batch).mean()\n",
    "                val_acc = (self.predict(X_val) == y_val).mean()\n",
    "                train_acc_history.append(train_acc)\n",
    "                val_acc_history.append(val_acc)\n",
    "                \n",
    "            # Decay learning rate\n",
    "            learning_rate *= learning_rate_decay\n",
    "\n",
    "        return {\n",
    "          'loss_history': loss_history,\n",
    "          'train_acc_history': train_acc_history,\n",
    "          'val_acc_history': val_acc_history,\n",
    "        }\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Входные параметры:\n",
    "        - X: Матрица данных (N, D)\n",
    "\n",
    "        Возвращает:\n",
    "        - y_pred: Вектор предсказаний классов для обьектов (N,)\n",
    "        \"\"\"\n",
    "        y_pred = None\n",
    "\n",
    "        ###########################################################################\n",
    "        # TODO: Предсказание классов для обьектов из X                                     #\n",
    "        ###########################################################################\n",
    "        pass\n",
    "        ###########################################################################\n",
    "        #                              END OF YOUR CODE                           #\n",
    "        ###########################################################################\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Инициализация простого примера. Данные и обьект модели\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model():\n",
    "    np.random.seed(0)\n",
    "    return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "def init_toy_data():\n",
    "    np.random.seed(1)\n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    y = np.array([0, 1, 2, 2, 1])\n",
    "    return X, y\n",
    "\n",
    "net = init_toy_model()\n",
    "X, y = init_toy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Прямой проход, скор\n",
    "Реализуйте прямой проход в `TwoLayerNet.loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = net.loss(X)\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print()\n",
    "print('correct scores:')\n",
    "correct_scores = np.asarray([\n",
    "  [-0.81233741, -1.27654624, -0.70335995],\n",
    "  [-0.17129677, -1.18803311, -0.47310444],\n",
    "  [-0.51590475, -1.01354314, -0.8504215 ],\n",
    "  [-0.15419291, -0.48629638, -0.52901952],\n",
    "  [-0.00618733, -0.12435261, -0.15226949]])\n",
    "print(correct_scores)\n",
    "print()\n",
    "\n",
    "# The difference should be very small. We get < 1e-7\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(np.sum(np.abs(scores - correct_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Прямой проход, функция потерь\n",
    "\n",
    "В том же методе реализуйте вычисление функции потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss, _ = net.loss(X, y, reg=0.05)\n",
    "correct_loss = 1.30378789133\n",
    "\n",
    "# Ошибка должна быть < 1e-12\n",
    "print('Difference between your loss and correct loss:')\n",
    "print(np.sum(np.abs(loss - correct_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обратный проход\n",
    "Закончите реализацию метода, вычислением градиентов для `W1`, `b1`, `W2`, и `b2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n",
    "    \n",
    "    fx = f(x) # evaluate function value at original point\n",
    "    grad = np.zeros_like(x)\n",
    "    # iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "\n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h # increment by h\n",
    "        fxph = f(x) # evalute f(x + h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x) # evaluate f(x - h)\n",
    "        x[ix] = oldval # restore\n",
    "\n",
    "        # compute the partial derivative with centered formula\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
    "        if verbose:\n",
    "            print(ix, grad[ix])\n",
    "        it.iternext() # step to next dimension\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss, grads = net.loss(X, y, reg=0.05)\n",
    "\n",
    "# Ошибка должна быть меньше или около 1e-8\n",
    "for param_name in grads:\n",
    "    f = lambda W: net.loss(X, y, reg=0.05)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "    print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение сети\n",
    "Реализуйте метод `TwoLayerNet.train` и метод `TwoLayerNet.predict`\n",
    "\n",
    "После того как закончите, обучите сеть на игрушечных данных, которые мы сгенерировали выше, лосс дольжен быть менее или около 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = init_toy_model()\n",
    "stats = net.train(X, y, X, y,\n",
    "            learning_rate=1e-1, reg=5e-6,\n",
    "            num_iters=100, verbose=False)\n",
    "\n",
    "print('Final training loss: ', stats['loss_history'][-1])\n",
    "\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_val, y_val) = cifar10.load_data()\n",
    "\n",
    "X_test, y_test = X_val[:int(X_val.shape[0]*0.5)], y_val[:int(X_val.shape[0]*0.5)]\n",
    "X_val, y_val = X_val[int(X_val.shape[0]*0.5):], y_val[int(X_val.shape[0]*0.5):]\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение сети\n",
    "Обучите сеть на данных CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "stats = net.train(X_train, y_train, X_val, y_val,\n",
    "            num_iters=1000, batch_size=200,\n",
    "            learning_rate=1e-4, learning_rate_decay=0.95,\n",
    "            reg=0.25, verbose=True)\n",
    "\n",
    "val_acc = (net.predict(X_val) == y_val).mean()\n",
    "print('Validation accuracy: ', val_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Дебаггинг процесса обучния"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['train_acc_history'], label='train')\n",
    "plt.plot(stats['val_acc_history'], label='val')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Настройка гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_net = None\n",
    "\n",
    "#################################################################################\n",
    "# TODO: Напишите свою реализцию кросс валидации для настройки гиперпараметров сети       #\n",
    "#################################################################################\n",
    "pass\n",
    "#################################################################################\n",
    "#                               END OF YOUR CODE                                #\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверка качества\n",
    "С оптимальными гиперпараметрами сеть должна выдавать точнов около 48%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_acc = (best_net.predict(X_test) == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
